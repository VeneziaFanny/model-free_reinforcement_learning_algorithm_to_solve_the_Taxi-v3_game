{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impl√©menter Deep Q-Learning (DQN)\n",
    "\n",
    " DQN utilise un r√©seau de neurones pour approximer la Q-table, √©vitant ainsi de stocker toutes les valeurs possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Importation du module torch.nn de PyTorch, qui contient des classes et des fonctions pour construire des r√©seaux de neurones\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå √âtape 1 : Construire l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "state_dim = env.observation_space.n   # Nombre d'√©tats\n",
    "action_dim = env.action_space.n       # Nombre d'actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå √âtape 2 : Construire le R√©seau de Neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition du r√©seau de neurones (DQN)\n",
    "# D√©finition d'une classe DQN qui h√©rite de nn.Module : elle repr√©sente le r√©seau de neurones pour l'algorithme Deep Q-Learning\n",
    "# 3 couches : une d‚Äôentr√©e (√©tat), deux cach√©es, et une de sortie (actions possibles)\n",
    "class DQN(nn.Module):\n",
    "    # constructeur de la classe DQN,  Il prend en entr√©e la dimension de l'√©tat et le nombre d'actions possibles.\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # Premi√®re couche cach√©e et enti√®rement connect√©e, input_dim en entr√©e et 128 neurones en sortie\n",
    "        self.fc2 = nn.Linear(128, 128)        # Deuxi√®me couche cach√©e et enti√®rement connect√©e, 128 en entr√©e et 128 neurones en sortie\n",
    "        self.fc3 = nn.Linear(128, output_dim) # Sortie = vecteur Q(s,a) avec une valeur pour chaque action / couche de sortie avec 128 neurones en entr√©e et output_dim neurones en sortie\n",
    "\n",
    "    # D√©finition de la m√©thode forward qui sp√©cifie le passage avant (forward pass) du r√©seau de neurones\n",
    "    # Elle prend en entr√©e x, qui est l'√©tat actuel\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # On applique ReLU comme fonction d‚Äôactivation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "# Ce r√©seau de neurones est utilis√© dans l'algorithme Deep Q-Learning pour approximer la fonction de valeur d'action Q(s, a),\n",
    "# qui est utilis√©e pour prendre des d√©cisions optimales dans un environnement donn√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå √âtape 3 : Entra√Æner le mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en vecteur one-hot pour l'entr√©e du r√©seau\n",
    "def one_hot_encoding(state, state_dim):\n",
    "    state_vec = np.zeros(state_dim)\n",
    "    state_vec[state] = 1\n",
    "    return state_vec\n",
    "\n",
    "# Initialisation du mod√®le DQN et de l'optimiseur\n",
    "model = DQN(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()  # Erreur quadratique moyenne\n",
    "\n",
    "# Param√®tres d'entra√Ænement\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay_rate = 0.005\n",
    "n_episodes = 2000 # 20000\n",
    "batch_size = 32\n",
    "replay_memory = []\n",
    "\n",
    "# Param√®tres √† ajouter : learning_rate, buffer_size, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fvenezia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state_vec = torch.tensor(one_hot_encoding(state, state_dim), dtype=torch.float32)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Exploration vs Exploitation\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_vec)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "        # Effectuer l'action\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        new_state_vec = torch.tensor(one_hot_encoding(new_state, state_dim), dtype=torch.float32)\n",
    "\n",
    "        # Stocker l'exp√©rience dans le buffer\n",
    "        replay_memory.append((state_vec, action, reward, new_state_vec, done))\n",
    "        if len(replay_memory) > 10000:  # Limite du buffer\n",
    "            replay_memory.pop(0)\n",
    "\n",
    "        # √âchantillonner un mini-lot et entra√Æner le r√©seau\n",
    "        if len(replay_memory) > batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, new_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.stack(states)\n",
    "            new_states = torch.stack(new_states)\n",
    "            actions = torch.tensor(actions)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            q_values = model(states)\n",
    "            next_q_values = model(new_states).detach()\n",
    "            target_q_values = q_values.clone()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                target_q_values[i, actions[i]] = rewards[i] + (1 - dones[i]) * gamma * torch.max(next_q_values[i])\n",
    "\n",
    "            loss = loss_fn(q_values, target_q_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Mise √† jour de l'√©tat\n",
    "        state_vec = new_state_vec\n",
    "\n",
    "    # Diminuer epsilon pour moins d'exploration\n",
    "    epsilon = max(min_epsilon, epsilon * np.exp(-decay_rate * episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå √âtape 4 : √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©compense moyenne : -17.01\n"
     ]
    }
   ],
   "source": [
    "def evaluate_dqn(env, model, n_episodes=100):\n",
    "    rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state_vec = torch.tensor(one_hot_encoding(state, state_dim), dtype=torch.float32)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_vec)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            state_vec = torch.tensor(one_hot_encoding(new_state, state_dim), dtype=torch.float32)\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    print(f\"R√©compense moyenne : {np.mean(rewards)}\")\n",
    "\n",
    "evaluate_dqn(env, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
